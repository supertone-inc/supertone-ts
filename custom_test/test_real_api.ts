#!/usr/bin/env node
/**
 * Real API Integration Test Script
 * Tests all SDK functionality with real Supertone API calls.
 *
 * Comprehensive TypeScript version of the Python test suite.
 * Includes all API tests except parallel processing tests.
 */

import * as fs from "fs";
import * as path from "path";
import { fileURLToPath } from "url";
import * as dotenv from "dotenv";

// Get the directory of the current module
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load environment variables from .env file in custom_test directory
dotenv.config({ path: path.join(__dirname, ".env") });

const API_KEY = process.env.SUPERTONE_API_KEY || "your-api-key-here";

interface TestResult {
	[key: string]: boolean | null;
}

/**
 * Helper function to log detailed error information
 */
function logDetailedError(e: any, context?: string): void {
	console.error(`  âŒ Error: ${e.message || e}`);

	if (context) {
		console.error(`  ğŸ“ Context: ${context}`);
	}

	// Validation errors (Zod)
	if (e.message && e.message.includes("validation")) {
		if (e.cause) {
			console.error(`  ğŸ“‹ Validation details:`);
			try {
				console.error(JSON.stringify(e.cause, null, 2));
			} catch {
				console.error(e.cause);
			}
		}
	}

	// HTTP errors
	if (e.statusCode || e.status) {
		console.error(`  ğŸŒ Status: ${e.statusCode || e.status}`);
	}

	// Response body if available
	if (e.body) {
		console.error(`  ğŸ“¦ Response body:`);
		try {
			console.error(JSON.stringify(e.body, null, 2));
		} catch {
			console.error(e.body);
		}
	}

	// Stack trace in debug mode
	if (process.env.DEBUG && e.stack) {
		console.error(`  ğŸ“š Stack trace:`);
		console.error(e.stack);
	}
}

/**
 * Helper function to inspect and log response object
 */
function inspectResponse(response: any, label: string = "Response"): void {
	console.log(`  ğŸ” ${label} inspection:`);
	console.log(`     Type: ${typeof response}`);
	console.log(`     Constructor: ${response?.constructor?.name || "unknown"}`);

	if (response) {
		const keys = Object.keys(response);
		console.log(`     Keys: ${keys.join(", ") || "(none)"}`);

		// Check for result property
		if ("result" in response) {
			const result = response.result;
			console.log(`     Result type: ${typeof result}`);
			console.log(
				`     Result constructor: ${result?.constructor?.name || "unknown"}`
			);

			if (result && typeof result === "object") {
				const resultKeys = Object.keys(result);
				console.log(
					`     Result keys: ${resultKeys.join(", ") || "(empty object)"}`
				);

				// Check prototype
				const proto = Object.getPrototypeOf(result);
				if (proto && proto.constructor) {
					console.log(`     Result prototype: ${proto.constructor.name}`);
				}

				// Check for stream methods
				if ("getReader" in result) {
					console.log(`     âœ“ Has getReader (ReadableStream-like)`);
				}
				if ("arrayBuffer" in result) {
					console.log(`     âœ“ Has arrayBuffer method`);
				}
			}
		}
	}
}

/**
 * Helper function to extract audio data from various response types
 */
async function extractAudioData(response: any): Promise<Uint8Array> {
	let result = response.result;

	// Debug logging
	console.log(`  ğŸ” Debug - result type: ${typeof result}`);
	if (typeof result === "object" && result !== null) {
		console.log(`  ğŸ” Debug - constructor: ${result.constructor.name}`);
		console.log(`  ğŸ” Debug - keys: ${Object.keys(result).join(", ")}`);
		console.log(`  ğŸ” Debug - has audioBase64: ${"audioBase64" in result}`);
		console.log(`  ğŸ” Debug - has getReader: ${"getReader" in result}`);
	}

	// Check for capital-case Result (SDK internal structure)
	if (
		!result ||
		(typeof result === "object" && Object.keys(result).length === 0)
	) {
		console.log(`  ğŸ’¡ Checking SDK internal Result field...`);
		if ((response as any).Result) {
			result = (response as any).Result;
			console.log(`  âœ… Found Result (capital R) - using that instead`);
		}
	}

	// Debug response headers
	if (response.headers) {
		console.log(
			`  ğŸ” Debug - response headers:`,
			JSON.stringify(response.headers, null, 2)
		);
	}

	if (result instanceof Uint8Array) {
		console.log(`  âœ… Detected: Uint8Array`);
		return result;
	}

	if (result instanceof Blob) {
		console.log(`  âœ… Detected: Blob`);
		return new Uint8Array(await result.arrayBuffer());
	}

	if (result instanceof ArrayBuffer) {
		console.log(`  âœ… Detected: ArrayBuffer`);
		return new Uint8Array(result);
	}

	if (typeof result === "object" && result !== null && "getReader" in result) {
		console.log(`  âœ… Detected: ReadableStream`);
		// ReadableStream
		const reader = (result as ReadableStream<Uint8Array>).getReader();
		const chunks: Uint8Array[] = [];

		while (true) {
			const { done, value } = await reader.read();
			if (done) break;
			if (value) chunks.push(value);
		}

		// Merge all chunks
		const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
		const merged = new Uint8Array(totalLength);
		let offset = 0;
		for (const chunk of chunks) {
			merged.set(chunk, offset);
			offset += chunk.length;
		}
		return merged;
	}

	// Handle CreateSpeechResponseBody with audioBase64 (for phoneme responses)
	if (
		typeof result === "object" &&
		result !== null &&
		"audioBase64" in result
	) {
		console.log(`  âœ… Detected: audioBase64 object`);
		const audioBase64 = (result as any).audioBase64;
		if (typeof audioBase64 === "string") {
			// Decode base64 to binary
			const binaryString = atob(audioBase64);
			const bytes = new Uint8Array(binaryString.length);
			for (let i = 0; i < binaryString.length; i++) {
				bytes[i] = binaryString.charCodeAt(i);
			}
			return bytes;
		}
	}

	// Handle empty object case - this might happen when the SDK doesn't properly parse audio responses
	if (
		typeof result === "object" &&
		result !== null &&
		Object.keys(result).length === 0
	) {
		console.log(`  âš ï¸  Warning: Empty result object detected`);
		console.log(`  ğŸ’¡ This might be a parsing issue with the SDK`);
		console.log(
			`  ğŸ’¡ Check if the response was actually a stream but got parsed as an empty object`
		);

		throw new Error(
			`Empty result object - SDK may have failed to parse audio stream response. ` +
				`This usually happens when audio/* content-type responses are not properly handled.`
		);
	}

	// Enhanced error message with debug info
	const errorDetails =
		typeof result === "object" && result !== null
			? `constructor: ${result.constructor.name}, keys: [${Object.keys(
					result
			  ).join(", ")}]`
			: `value: ${result}`;

	throw new Error(`Unsupported result type: ${typeof result}, ${errorDetails}`);
}

/**
 * Test credit balance retrieval
 */
async function testCreditBalance(): Promise<[boolean, any]> {
	console.log("ğŸ’° Credit Balance Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log("  ğŸ” Retrieving credit balance...");
		const response = await client.usage.getCreditBalance();

		console.log(`  âœ… Credit Balance: ${response.balance}`);
		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test usage retrieval
 */
async function testGetUsage(): Promise<[boolean, any]> {
	console.log("ğŸ“Š Usage Analytics Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const endTime = new Date();
		const startTime = new Date(endTime.getTime() - 7 * 24 * 60 * 60 * 1000);

		console.log(
			`  ğŸ” Retrieving usage from ${startTime.toISOString().split("T")[0]} to ${
				endTime.toISOString().split("T")[0]
			}...`
		);

		const response = await client.usage.getUsage({
			startTime: startTime.toISOString(),
			endTime: endTime.toISOString(),
		});

		console.log(
			`  âœ… Success: ${response.data?.length || 0} usage record buckets`
		);
		console.log(`  ğŸ“Š Total buckets: ${response.total}`);

		if (response.data && response.data.length > 0) {
			for (const bucket of response.data.slice(0, 3)) {
				console.log(`  ğŸ“… Bucket start: ${bucket.startingAt}`);
				console.log(`     Bucket end: ${bucket.endingAt}`);
				console.log(`     Results: ${bucket.results?.length || 0} items`);

				if (bucket.results) {
					const totalMinutes = bucket.results.reduce(
						(sum, r) => sum + (r.minutesUsed || 0),
						0
					);
					console.log(`     Total usage: ${totalMinutes.toFixed(2)} minutes`);
				}
			}
		} else {
			console.log("  ğŸ“ No usage records for this period");
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test voice listing
 */
async function testListVoices(): Promise<[boolean, any]> {
	console.log("ğŸµ Voice List Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log("  ğŸ” Retrieving voice list...");
		const response = await client.voices.listVoices({ pageSize: 10 });

		console.log(`  âœ… Success: ${response.items?.length || 0} voices`);
		console.log(`  ğŸ“Š Total voices: ${response.total}`);

		let firstVoiceId: string | null = null;
		if (response.items && response.items.length > 0) {
			const firstVoice = response.items[0];
			firstVoiceId = firstVoice!.voiceId || null;
			console.log(`  ğŸ¤ First voice:`);
			console.log(`     ID: ${firstVoice!.voiceId}`);
			console.log(`     Name: ${firstVoice!.name}`);
			console.log(
				`     Description: ${firstVoice!.description?.substring(0, 50)}...`
			);
			console.log(`     Language: ${firstVoice!.language}`);
			console.log(`     Gender: ${firstVoice!.gender}`);
		}

		return [true, { response, voiceId: firstVoiceId }];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test voice search
 */
async function testSearchVoices(): Promise<[boolean, any]> {
	console.log("ğŸ” Voice Search Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log("  ğŸ” Searching for female English voices...");
		const response = await client.voices.searchVoices({
			language: "en",
			gender: "female",
			pageSize: 10,
		});

		console.log(`  âœ… Search success: ${response.items?.length || 0} voices`);

		if (response.items) {
			for (const voice of response.items.slice(0, 5)) {
				console.log(`  ğŸ¤ ${voice.name} (${voice.voiceId})`);
				console.log(
					`     Language: ${voice.language}, Gender: ${voice.gender}`
				);
			}
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test voice detail retrieval
 */
async function testGetVoice(voiceId: string | null): Promise<[boolean, any]> {
	console.log("ğŸ“„ Voice Detail Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” Retrieving voice '${voiceId}' details...`);
		const response = await client.voices.getVoice({ voiceId });

		console.log(`  âœ… Success:`);
		console.log(`     Name: ${response.name}`);
		console.log(`     ID: ${response.voiceId}`);
		console.log(`     Description: ${response.description}`);
		console.log(`     Language: ${response.language}`);
		console.log(`     Gender: ${response.gender}`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test custom voice listing
 */
async function testListCustomVoices(): Promise<[boolean, any]> {
	console.log("ğŸ¨ Custom Voice List Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log("  ğŸ” Retrieving custom voice list...");
		const response = await client.customVoices.listCustomVoices({
			pageSize: 10,
		});

		console.log(`  âœ… Success: ${response.items?.length || 0} custom voices`);
		console.log(`  ğŸ“Š Total custom voices: ${response.total}`);

		let customVoiceId: string | null = null;
		if (response.items) {
			for (const voice of response.items) {
				console.log(`  ğŸ¤ ${voice.name} (${voice.voiceId})`);
				console.log(`     Description: ${voice.description}`);
				if (!customVoiceId) {
					customVoiceId = voice.voiceId || null;
				}
			}
		}

		return [true, { response, voiceId: customVoiceId }];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS duration prediction
 */
async function testPredictDuration(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("â±ï¸  TTS Duration Prediction Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "Hello, this is a test message for duration prediction.";
		console.log(`  ğŸ” Predicting duration for: "${testText}"`);

		const response = await client.textToSpeech.predictDuration({
			voiceId,
			predictTTSDurationUsingCharacterRequest: {
				text: testText,
				language: models.PredictTTSDurationUsingCharacterRequestLanguage.En,
			},
		});

		console.log(`  âœ… Predicted duration: ${response.duration}s`);
		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS creation (WAV)
 */
async function testCreateSpeech(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ¤ TTS Creation Test (WAV)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "Hello! This is a test speech synthesis.";
		console.log(`  ğŸ” Creating speech: "${testText}"`);
		console.log(`     Voice ID: ${voiceId}`);
		console.log(`     Format: WAV`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: testText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
			},
		});

		console.log(`  âœ… Speech created successfully`);
		console.log(`     Audio size: ${response.result ? "received" : "none"}`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS streaming
 */
async function testStreamSpeech(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“¡ TTS Streaming Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "Testing streaming speech synthesis.";
		console.log(`  ğŸ” Streaming speech: "${testText}"`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.streamSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: testText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
			},
		});

		console.log(`  âœ… Stream started successfully`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test voice-specific usage retrieval
 */
async function testGetVoiceUsage(): Promise<[boolean, any]> {
	console.log("ğŸ¤ Voice Usage Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const endDate = new Date();
		const startDate = new Date(endDate.getTime() - 7 * 24 * 60 * 60 * 1000);

		console.log(
			`  ğŸ” Retrieving voice usage from ${
				startDate.toISOString().split("T")[0]
			} to ${endDate.toISOString().split("T")[0]}...`
		);

		const response = await client.usage.getVoiceUsage({
			startDate: startDate.toISOString().split("T")[0],
			endDate: endDate.toISOString().split("T")[0],
		});

		console.log(
			`  âœ… Success: ${response.usages?.length || 0} voice usage records`
		);

		if (response.usages && response.usages.length > 0) {
			for (const usage of response.usages.slice(0, 5)) {
				const voiceName =
					usage.name || `Voice ${usage.voiceId?.substring(0, 8) || "Unknown"}`;
				console.log(
					`  ğŸ¤ ${voiceName}: ${usage.totalMinutesUsed?.toFixed(2)}min`
				);
				console.log(`     Voice ID: ${usage.voiceId}`);
				if (usage.language) {
					console.log(`     Language: ${usage.language}`);
				}
			}
		} else {
			console.log("  ğŸ“ No voice usage records for this period");
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test custom voice search
 */
async function testSearchCustomVoices(): Promise<[boolean, any]> {
	console.log("ğŸ” Custom Voice Search Test");

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log("  ğŸ” Searching custom voices...");
		const response = await client.customVoices.searchCustomVoices({
			pageSize: 10,
		});

		console.log(
			`  âœ… Search success: ${response.items?.length || 0} custom voices`
		);

		if (response.items) {
			for (const voice of response.items) {
				console.log(`  ğŸ¤ ${voice.name} (${voice.voiceId})`);
				console.log(`     Description: ${voice.description}`);
			}
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test custom voice detail retrieval
 */
async function testGetCustomVoice(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“„ Custom Voice Detail Test");

	if (!voiceId) {
		console.log("  âš ï¸  No custom voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” Retrieving custom voice '${voiceId}' details...`);
		const response = await client.customVoices.getCustomVoice({ voiceId });

		console.log(`  âœ… Success:`);
		console.log(`     Name: ${response.name}`);
		console.log(`     ID: ${response.voiceId}`);
		console.log(`     Description: ${response.description}`);

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "Get custom voice");
		return [false, e];
	}
}

/**
 * Test custom voice creation
 */
async function testCreateClonedVoice(): Promise<[boolean, any]> {
	console.log("ğŸ¨ Custom Voice Creation Test");

	// File is in custom_test directory
	const audioFilePath = path.join(__dirname, "voice_sample.wav");

	if (!fs.existsSync(audioFilePath)) {
		console.log(`  âŒ Audio file not found: ${audioFilePath}`);
		console.log(`  â­ï¸  Skipping test (not a failure)`);
		return [true, null]; // Return success with null to skip, not fail
	}

	console.log(`  ğŸ“ Using audio file: ${audioFilePath}`);

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const stats = fs.statSync(audioFilePath);
		const fileSize = stats.size;
		const maxSize = 3 * 1024 * 1024; // 3MB

		console.log(
			`  ğŸ“ File size: ${fileSize.toLocaleString()} bytes (${(
				fileSize /
				1024 /
				1024
			).toFixed(2)} MB)`
		);

		if (fileSize > maxSize) {
			console.log(
				`  âŒ File exceeds 3MB limit: ${(fileSize / 1024 / 1024).toFixed(2)} MB`
			);
			return [false, null];
		}

		const timestamp = new Date()
			.toLocaleString("en-US", {
				month: "2-digit",
				day: "2-digit",
				hour: "2-digit",
				minute: "2-digit",
				hour12: false,
			})
			.replace(/[/, :]/g, "");
		const voiceName = `Test Sample Voice ${timestamp}`;
		const voiceDescription = `Test custom voice created at ${new Date().toISOString()}`;

		console.log(`  ğŸ” Creating custom voice...`);
		console.log(`     File: ${audioFilePath}`);
		console.log(`     Name: ${voiceName}`);
		console.log(`     Description: ${voiceDescription}`);
		console.log(
			"  âš ï¸  This test consumes credits and creates actual custom voice!"
		);

		const audioContent = fs.readFileSync(audioFilePath);

		const response = await client.customVoices.createClonedVoice({
			files: {
				fileName: "voice_sample.wav",
				content: audioContent,
			},
			name: voiceName,
			description: voiceDescription,
		});

		console.log(`  âœ… Custom voice creation request successful!`);
		console.log(`     Voice ID: ${response.voiceId}`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test custom voice update
 */
async function testEditCustomVoice(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("âœï¸  Custom Voice Update Test");

	if (!voiceId) {
		console.log("  âš ï¸  No custom voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const timestamp = new Date().toISOString();
		const testName = `Updated Test Voice ${timestamp}`;
		const testDescription = `Updated description at ${timestamp}`;

		console.log(`  ğŸ”„ Updating custom voice '${voiceId}'...`);
		console.log(`     New name: ${testName}`);
		console.log(`     New description: ${testDescription}`);

		const response = await client.customVoices.editCustomVoice({
			voiceId,
			updateClonedVoiceRequest: {
				name: testName,
				description: testDescription,
			},
		});

		console.log(`  âœ… Custom voice updated successfully`);
		console.log(`     Name: ${response.name}`);
		console.log(`     Description: ${response.description}`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test custom voice deletion
 */
async function testDeleteCustomVoice(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ—‘ï¸  Custom Voice Deletion Test");

	if (!voiceId) {
		console.log("  âš ï¸  No custom voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” Deleting custom voice '${voiceId}'...`);
		console.log("  âš ï¸  This test permanently deletes the custom voice!");

		const response = await client.customVoices.deleteCustomVoice({ voiceId });

		console.log(`  âœ… Custom voice deleted successfully`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS with long text (auto-chunking at SDK level)
 */
async function testCreateSpeechLongText(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“œ Long Text Auto-Chunking TTS Test (300+ chars)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const longText = `
		Hello! This is a very long text auto-chunking TTS test exceeding 300 characters.
		The newly implemented SDK automatically divides long text into multiple chunks for processing.
		Real-time streaming text-to-speech technology plays a crucial role in modern AI applications.
		It is an indispensable technology especially in conversational services, live broadcasting, and real-time translation services.
		Through the auto-chunking feature, long texts are naturally divided into multiple small segments for processing.
		Each segment is intelligently segmented considering sentence and word boundaries, enabling natural speech generation.
		Now users don't need to worry about text length, as the SDK automatically handles everything.
		`.trim();

		const actualLength = longText.length;
		console.log(
			`  ğŸ“ Test text length: ${actualLength} characters (exceeds 300)`
		);
		console.log(`  ğŸ”§ Auto-chunking enabled for text segmentation`);

		console.log(`  ğŸ” Converting long text with voice '${voiceId}'...`);
		console.log("  âš ï¸  This test consumes credits!");
		console.log("  âœ¨ SDK automatically chunks and processes the text");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				style: "neutral",
				model: "sona_speech_1",
			},
		});

		if (response.result) {
			let audioSize = 0;
			let audioData: Uint8Array;

			if (response.result instanceof Uint8Array) {
				audioSize = response.result.length;
				audioData = response.result;
			} else if (response.result instanceof Blob) {
				audioSize = response.result.size;
				audioData = new Uint8Array(await response.result.arrayBuffer());
			} else if (
				typeof response.result === "object" &&
				"getReader" in response.result
			) {
				// ReadableStream
				audioData = await extractAudioData(response);
				audioSize = audioData.length;
			} else {
				audioData = new Uint8Array(0);
			}

			console.log(
				`  âœ… Auto-chunking TTS success: ${audioSize} bytes audio generated`
			);
			console.log(`  ğŸ¯ Long text successfully chunked and processed!`);

			const outputFile = "test_auto_chunking_speech_output.wav";
			fs.writeFileSync(outputFile, audioData);
			console.log(`  ğŸ’¾ Auto-chunked audio file saved: ${outputFile}`);

			const estimatedChunks = Math.ceil(actualLength / 300);
			console.log(
				`  ğŸ“Š Estimated chunks: ${estimatedChunks} (based on text length)`
			);
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS with long text WITHOUT punctuation (word-based chunking)
 * This tests the word-based splitting fallback when sentences exceed 300 chars
 */
async function testCreateSpeechLongSentenceNoPunctuation(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log(
		"ğŸ“œ Long Sentence WITHOUT Punctuation Test (Word-based chunking)"
	);

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		// Long text without punctuation - forces word-based splitting
		// This is a single continuous sentence with no periods or other punctuation marks
		const longSentenceNoPunctuation =
			"This is a very long sentence without any punctuation marks that is designed to test the word based chunking feature of the SDK when a sentence exceeds the maximum character limit of three hundred characters the system should automatically split this text by word boundaries rather than sentence boundaries to ensure proper processing and this behavior is critical for handling user generated content that may not follow standard punctuation conventions such as chat messages or informal text inputs that users commonly provide in real world applications where grammatically correct sentences are not always guaranteed";

		const actualLength = longSentenceNoPunctuation.length;
		console.log(
			`  ğŸ“ Text length: ${actualLength} characters (single sentence, no punctuation)`
		);
		console.log(`  ğŸ”§ Expected behavior: Word-based chunking`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longSentenceNoPunctuation,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				style: "neutral",
				model: "sona_speech_1",
			},
		});

		if (response.result) {
			const audioData = await extractAudioData(response);

			console.log(
				`  âœ… Word-based chunking TTS success: ${audioData.length} bytes`
			);
			console.log(
				`  ğŸ¯ Long sentence without punctuation processed correctly!`
			);

			const outputFile = "test_word_chunking_speech_output.wav";
			fs.writeFileSync(outputFile, audioData);
			console.log(`  ğŸ’¾ Audio saved: ${outputFile}`);

			const estimatedChunks = Math.ceil(actualLength / 300);
			console.log(`  ğŸ“Š Estimated chunks: ${estimatedChunks}`);
		}

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "Long sentence word-based chunking");
		return [false, e];
	}
}

/**
 * Test TTS with Japanese text (character-based chunking)
 * Japanese doesn't use spaces, AND this test uses NO punctuation marks (ã€‚ï¼ï¼Ÿetc)
 * to ensure the SDK uses character-based splitting
 */
async function testCreateSpeechJapaneseNoSpaces(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ‡¯ğŸ‡µ Japanese Text Test (Character-based chunking)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		// Long Japanese text WITHOUT spaces AND WITHOUT punctuation - forces character-based splitting
		// This text intentionally has NO punctuation marks (ã€‚ï¼ï¼Ÿetc) to test pure character-based chunking
		// Text length: ~450 characters (exceeds 300 char limit)
		const longJapaneseText =
			"æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã¯é€šå¸¸ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã„ãŸã‚ç‰¹åˆ¥ãªå‡¦ç†ãŒå¿…è¦ã§ã™" +
			"ã“ã®ãƒ†ã‚¹ãƒˆã¯ä¸‰ç™¾æ–‡å­—ã‚’è¶…ãˆã‚‹é•·ã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆãŒæ­£ã—ãå‡¦ç†ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™" +
			"è‡ªç„¶è¨€èªå‡¦ç†æŠ€è¡“ã®ç™ºå±•ã«ã‚ˆã‚ŠéŸ³å£°åˆæˆã®å“è³ªã¯å¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸ" +
			"ç‰¹ã«ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ´»ç”¨ã—ãŸæœ€æ–°ã®ãƒ†ã‚­ã‚¹ãƒˆéŸ³å£°å¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ã¯äººé–“ã®ç™ºè©±ã«éå¸¸ã«è¿‘ã„è‡ªç„¶ãªéŸ³å£°ã‚’ç”Ÿæˆã§ãã¾ã™" +
			"ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„è¨€èªã§ã¯æ–‡å­—å˜ä½ã§ã®åˆ†å‰²ãŒå¿…è¦ã§ã‚ã‚Šã“ã®SDKã¯ãã®ã‚ˆã†ãªçŠ¶æ³ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã¦é©åˆ‡ã«å‡¦ç†ã—ã¾ã™" +
			"ã“ã‚Œã«ã‚ˆã‚Šæ—¥æœ¬èªä¸­å›½èªéŸ“å›½èªãªã©ã®ã‚¢ã‚¸ã‚¢è¨€èªã§ã‚‚å•é¡Œãªãé•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™" +
			"éŸ³å£°åˆæˆæŠ€è¡“ã¯è¦–è¦šéšœå®³è€…ã®ãŸã‚ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã‹ã‚‰å¯¾è©±å‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¾ã§å¹…åºƒã„ç”¨é€”ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™" +
			"ã•ã‚‰ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æŠ€è¡“ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å¾…ã¡æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã—å„ªã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™" +
			"æœ€æ–°ã®éŸ³å£°åˆæˆæŠ€è¡“ã¯æ„Ÿæƒ…ã‚„æŠ‘æšã‚‚è‡ªç„¶ã«è¡¨ç¾ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸ";

		const actualLength = longJapaneseText.length;
		console.log(
			`  ğŸ“ Text length: ${actualLength} characters (Japanese, no spaces, no punctuation)`
		);
		console.log(
			`  ğŸ”§ Expected behavior: Character-based chunking (300 chars per chunk)`
		);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longJapaneseText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.Ja,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				style: "neutral",
				model: "sona_speech_1",
			},
		});

		if (response.result) {
			const audioData = await extractAudioData(response);

			console.log(
				`  âœ… Character-based chunking TTS success: ${audioData.length} bytes`
			);
			console.log(`  ğŸ¯ Japanese text without spaces processed correctly!`);

			const outputFile = "test_japanese_char_chunking_speech_output.wav";
			fs.writeFileSync(outputFile, audioData);
			console.log(`  ğŸ’¾ Audio saved: ${outputFile}`);

			const estimatedChunks = Math.ceil(actualLength / 300);
			console.log(`  ğŸ“Š Estimated chunks: ${estimatedChunks}`);
		}

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "Japanese character-based chunking");
		return [false, e];
	}
}

/**
 * Test TTS streaming with long text
 */
async function testStreamSpeechLongText(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“¡ Long Text Streaming TTS Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const longText = `
		Hello! This is a long text streaming test.
		The SDK automatically chunks and streams the audio in real-time.
		This enables efficient processing of longer content without waiting for complete generation.
		`
			.trim()
			.repeat(3);

		console.log(`  ğŸ” Streaming long text with voice '${voiceId}'...`);
		console.log(`     Text length: ${longText.length} characters`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.streamSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
			},
		});

		console.log(`  âœ… Stream started successfully`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS with voice settings
 */
async function testCreateSpeechWithVoiceSettings(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ›ï¸  TTS with Voice Settings Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const voiceSettings = {
			pitchShift: 0.95,
			pitchVariance: 1.1,
			speed: 0.9,
		};

		console.log(
			`  ğŸ” TTS conversion with voice settings using voice '${voiceId}'...`
		);
		console.log(
			`     Settings: pitchShift=${voiceSettings.pitchShift}, speed=${voiceSettings.speed}`
		);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Hello world! This is a voice settings test. You can hear the adjusted pitch and speed.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				style: "neutral",
				model: "sona_speech_1",
				voiceSettings,
				includePhonemes: false,
			},
		});

		console.log(`  âœ… TTS with voice settings success`);

		if (response.result) {
			const outputFile = "test_voice_settings_speech_output.wav";
			const audioData = await extractAudioData(response);

			fs.writeFileSync(outputFile, audioData);
			console.log(`  ğŸ’¾ Voice settings audio file saved: ${outputFile}`);
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS with phoneme information
 */
async function testCreateSpeechWithPhonemes(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ”¤ TTS with Phoneme Information Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(
			`  ğŸ” TTS conversion with phonemes using voice '${voiceId}'...`
		);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Hello world! This is a phoneme timing test.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				style: "neutral",
				model: "sona_speech_1",
				includePhonemes: true,
			},
		});

		console.log(`  âœ… TTS with phonemes success`);

		if (response.result) {
			const outputFile = "test_phoneme_speech_output.wav";

			// Check if response is JSON with phonemes data
			if (
				typeof response.result === "object" &&
				"audioBase64" in response.result
			) {
				const audioData = await extractAudioData(response);
				fs.writeFileSync(outputFile, audioData);
				console.log(`  ğŸ’¾ Phoneme audio file saved: ${outputFile}`);

				// Display phoneme information as JSON
				const phonemes = (response.result as any).phonemes;
				if (phonemes) {
					console.log(`  ğŸ“Š Phoneme data (JSON):`);
					console.log(JSON.stringify(phonemes, null, 2));
					console.log(`  ğŸ“ˆ Summary:`);
					console.log(`     Symbols count: ${phonemes.symbols?.length || 0}`);
					console.log(
						`     Durations count: ${phonemes.durations_seconds?.length || 0}`
					);
					console.log(
						`     Start times count: ${
							phonemes.start_times_seconds?.length || 0
						}`
					);
					if (phonemes.symbols && phonemes.symbols.length > 0) {
						console.log(
							`     First 5 symbols: ${phonemes.symbols.slice(0, 5).join(", ")}`
						);
					}
				}
			} else {
				// Binary audio without phonemes
				const audioData = await extractAudioData(response);
				fs.writeFileSync(outputFile, audioData);
				console.log(`  ğŸ’¾ Phoneme audio file saved: ${outputFile}`);
			}
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS streaming with phonemes
 */
async function testStreamSpeechWithPhonemes(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“¡ TTS Streaming with Phonemes Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(
			`  ğŸ” Streaming speech with phonemes for voice '${voiceId}'...`
		);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.streamSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Streaming with phoneme timing information.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				includePhonemes: true,
			},
		});

		console.log(`  âœ… Stream with phonemes started successfully`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

// =============================================================================
// Model & Language Compatibility Tests
// =============================================================================

/**
 * Model-Language compatibility matrix
 * - sona_speech_1: ko, en, ja
 * - sona_speech_2: all languages (23 languages)
 * - supertonic_api_1: ko, en, ja, es, pt
 */
const MODEL_LANGUAGE_MATRIX = {
	sona_speech_1: ["ko", "en", "ja"],
	sona_speech_2: [
		"en",
		"ko",
		"ja",
		"bg",
		"cs",
		"da",
		"el",
		"es",
		"et",
		"fi",
		"hu",
		"it",
		"nl",
		"pl",
		"pt",
		"ro",
		"ar",
		"de",
		"fr",
		"hi",
		"id",
		"ru",
		"vi",
	],
	supertonic_api_1: ["ko", "en", "ja", "es", "pt"],
} as const;

/**
 * Test TTS with sona_speech_2 model
 */
async function testCreateSpeechWithSonaSpeech2(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ¤– TTS with sona_speech_2 Model Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText =
			"Hello! Testing sona_speech_2 model for text-to-speech conversion.";
		console.log(`  ğŸ” Creating speech with sona_speech_2 model`);
		console.log(`     Voice ID: ${voiceId}`);
		console.log(`     Model: sona_speech_2`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: testText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				model:
					models.APIConvertTextToSpeechUsingCharacterRequestModel.SonaSpeech2,
			},
		});

		console.log(`  âœ… sona_speech_2 TTS success`);

		if (response.result) {
			const audioData = await extractAudioData(response);
			const outputFile = "test_sona_speech_2_output.wav";
			fs.writeFileSync(outputFile, audioData);
			console.log(
				`  ğŸ’¾ Audio saved: ${outputFile} (${audioData.length} bytes)`
			);
		}

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "sona_speech_2 TTS");
		return [false, e];
	}
}

/**
 * Test TTS with supertonic_api_1 model
 */
async function testCreateSpeechWithSupertonicApi1(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ¤– TTS with supertonic_api_1 Model Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText =
			"Hello! Testing supertonic_api_1 model for text-to-speech conversion.";
		console.log(`  ğŸ” Creating speech with supertonic_api_1 model`);
		console.log(`     Voice ID: ${voiceId}`);
		console.log(`     Model: supertonic_api_1`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: testText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				model:
					models.APIConvertTextToSpeechUsingCharacterRequestModel
						.SupertonicApi1,
			},
		});

		console.log(`  âœ… supertonic_api_1 TTS success`);

		if (response.result) {
			const audioData = await extractAudioData(response);
			const outputFile = "test_supertonic_api_1_output.wav";
			fs.writeFileSync(outputFile, audioData);
			console.log(
				`  ğŸ’¾ Audio saved: ${outputFile} (${audioData.length} bytes)`
			);
		}

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "supertonic_api_1 TTS");
		return [false, e];
	}
}

/**
 * Test TTS with unsupported model (should fail with validation error)
 */
async function testCreateSpeechWithUnsupportedModel(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸš« TTS with Unsupported Model Test (Expected to Fail)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "This should fail with unsupported model.";
		console.log(
			`  ğŸ” Attempting TTS with unsupported model: 'invalid_model_xyz'`
		);

		// Using type assertion to bypass TypeScript validation for testing
		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: testText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				model: "invalid_model_xyz" as any, // Intentionally invalid model
			},
		});

		// If we reach here, the test failed (should have thrown an error)
		console.log(`  âŒ Expected error but got success - this is unexpected!`);
		return [false, response];
	} catch (e: any) {
		// Expected to fail - this is the success case for this test
		console.log(`  âœ… Correctly rejected unsupported model`);
		console.log(`  ğŸ“‹ Error type: ${e.constructor?.name || typeof e}`);
		console.log(`  ğŸ“‹ Error message: ${e.message?.substring(0, 100) || e}`);
		return [true, e];
	}
}

/**
 * Test prediction with sona_speech_2 model
 */
async function testPredictDurationWithSonaSpeech2(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("â±ï¸  Duration Prediction with sona_speech_2 Model Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "Testing duration prediction with sona_speech_2 model.";
		console.log(`  ğŸ” Predicting duration with sona_speech_2 model`);

		const response = await client.textToSpeech.predictDuration({
			voiceId,
			predictTTSDurationUsingCharacterRequest: {
				text: testText,
				language: models.PredictTTSDurationUsingCharacterRequestLanguage.En,
				model: models.PredictTTSDurationUsingCharacterRequestModel.SonaSpeech2,
			},
		});

		console.log(
			`  âœ… sona_speech_2 duration prediction: ${response.duration}s`
		);
		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "sona_speech_2 duration prediction");
		return [false, e];
	}
}

/**
 * Test prediction with supertonic_api_1 model
 */
async function testPredictDurationWithSupertonicApi1(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("â±ï¸  Duration Prediction with supertonic_api_1 Model Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "Testing duration prediction with supertonic_api_1 model.";
		console.log(`  ğŸ” Predicting duration with supertonic_api_1 model`);

		const response = await client.textToSpeech.predictDuration({
			voiceId,
			predictTTSDurationUsingCharacterRequest: {
				text: testText,
				language: models.PredictTTSDurationUsingCharacterRequestLanguage.En,
				model:
					models.PredictTTSDurationUsingCharacterRequestModel.SupertonicApi1,
			},
		});

		console.log(
			`  âœ… supertonic_api_1 duration prediction: ${response.duration}s`
		);
		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "supertonic_api_1 duration prediction");
		return [false, e];
	}
}

/**
 * Test prediction with unsupported model (should fail with validation error)
 */
async function testPredictDurationWithUnsupportedModel(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log(
		"ğŸš« Duration Prediction with Unsupported Model Test (Expected to Fail)"
	);

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const testText = "This should fail with unsupported model.";
		console.log(
			`  ğŸ” Attempting prediction with unsupported model: 'invalid_model_xyz'`
		);

		const response = await client.textToSpeech.predictDuration({
			voiceId,
			predictTTSDurationUsingCharacterRequest: {
				text: testText,
				language: models.PredictTTSDurationUsingCharacterRequestLanguage.En,
				model: "invalid_model_xyz" as any, // Intentionally invalid model
			},
		});

		console.log(`  âŒ Expected error but got success - this is unexpected!`);
		return [false, response];
	} catch (e: any) {
		console.log(`  âœ… Correctly rejected unsupported model`);
		console.log(`  ğŸ“‹ Error type: ${e.constructor?.name || typeof e}`);
		console.log(`  ğŸ“‹ Error message: ${e.message?.substring(0, 100) || e}`);
		return [true, e];
	}
}

// =============================================================================
// Multilingual Tests per Model
// =============================================================================

/**
 * Test TTS multilingual support with sona_speech_1 (supports: ko, en, ja)
 */
async function testMultilingualSonaSpeech1(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸŒ Multilingual Test - sona_speech_1 (ko, en, ja)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	const testCases = [
		{
			lang: "ko" as const,
			text: "ì•ˆë…•í•˜ì„¸ìš”, ì†Œë‚˜ ìŠ¤í”¼ì¹˜ ì› ëª¨ë¸ì…ë‹ˆë‹¤.",
			label: "Korean",
		},
		{
			lang: "en" as const,
			text: "Hello, this is sona_speech_1 model.",
			label: "English",
		},
		{
			lang: "ja" as const,
			text: "ã“ã‚“ã«ã¡ã¯ã€ã‚½ãƒŠã‚¹ãƒ”ãƒ¼ãƒãƒ¯ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
			label: "Japanese",
		},
	];

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		let allPassed = true;
		const results: any[] = [];

		for (const tc of testCases) {
			console.log(`  ğŸ” Testing ${tc.label} (${tc.lang})...`);

			try {
				const langEnum =
					models.APIConvertTextToSpeechUsingCharacterRequestLanguage[
						(tc.lang.charAt(0).toUpperCase() +
							tc.lang.slice(
								1
							)) as keyof typeof models.APIConvertTextToSpeechUsingCharacterRequestLanguage
					];

				const response = await client.textToSpeech.createSpeech({
					voiceId,
					apiConvertTextToSpeechUsingCharacterRequest: {
						text: tc.text,
						language: langEnum,
						outputFormat:
							models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat
								.Wav,
						model:
							models.APIConvertTextToSpeechUsingCharacterRequestModel
								.SonaSpeech1,
					},
				});

				console.log(`     âœ… ${tc.label} success`);
				results.push({ lang: tc.lang, success: true });
			} catch (e: any) {
				console.log(
					`     âŒ ${tc.label} failed: ${e.message?.substring(0, 50)}`
				);
				results.push({ lang: tc.lang, success: false, error: e.message });
				allPassed = false;
			}
		}

		console.log(
			`  ğŸ“Š Result: ${results.filter((r) => r.success).length}/${
				testCases.length
			} languages passed`
		);
		return [allPassed, results];
	} catch (e: any) {
		logDetailedError(e, "sona_speech_1 multilingual");
		return [false, e];
	}
}

/**
 * Test TTS multilingual support with sona_speech_2 (supports all languages)
 */
async function testMultilingualSonaSpeech2(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸŒ Multilingual Test - sona_speech_2 (all languages sample)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	// Test a diverse subset of languages
	const testCases = [
		{ lang: "Ko" as const, text: "ì•ˆë…•í•˜ì„¸ìš”.", label: "Korean" },
		{ lang: "En" as const, text: "Hello.", label: "English" },
		{ lang: "Ja" as const, text: "ã“ã‚“ã«ã¡ã¯ã€‚", label: "Japanese" },
		{ lang: "Es" as const, text: "Hola.", label: "Spanish" },
		{ lang: "Fr" as const, text: "Bonjour.", label: "French" },
		{ lang: "De" as const, text: "Hallo.", label: "German" },
		{ lang: "Ar" as const, text: "Ù…Ø±Ø­Ø¨Ø§.", label: "Arabic" },
		{ lang: "Hi" as const, text: "à¤¨à¤®à¤¸à¥à¤¤à¥‡à¥¤", label: "Hindi" },
	];

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		let allPassed = true;
		const results: any[] = [];

		for (const tc of testCases) {
			console.log(`  ğŸ” Testing ${tc.label} (${tc.lang})...`);

			try {
				const langEnum =
					models.APIConvertTextToSpeechUsingCharacterRequestLanguage[tc.lang];

				const response = await client.textToSpeech.createSpeech({
					voiceId,
					apiConvertTextToSpeechUsingCharacterRequest: {
						text: tc.text,
						language: langEnum,
						outputFormat:
							models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat
								.Wav,
						model:
							models.APIConvertTextToSpeechUsingCharacterRequestModel
								.SonaSpeech2,
					},
				});

				console.log(`     âœ… ${tc.label} success`);
				results.push({ lang: tc.lang, success: true });
			} catch (e: any) {
				console.log(
					`     âŒ ${tc.label} failed: ${e.message?.substring(0, 50)}`
				);
				results.push({ lang: tc.lang, success: false, error: e.message });
				allPassed = false;
			}
		}

		console.log(
			`  ğŸ“Š Result: ${results.filter((r) => r.success).length}/${
				testCases.length
			} languages passed`
		);
		return [allPassed, results];
	} catch (e: any) {
		logDetailedError(e, "sona_speech_2 multilingual");
		return [false, e];
	}
}

/**
 * Test TTS multilingual support with supertonic_api_1 (supports: ko, en, ja, es, pt)
 */
async function testMultilingualSupertonicApi1(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸŒ Multilingual Test - supertonic_api_1 (ko, en, ja, es, pt)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	const testCases = [
		{
			lang: "Ko" as const,
			text: "ì•ˆë…•í•˜ì„¸ìš”, ìŠˆí¼í† ë‹‰ API ì› ëª¨ë¸ì…ë‹ˆë‹¤.",
			label: "Korean",
		},
		{
			lang: "En" as const,
			text: "Hello, this is supertonic_api_1 model.",
			label: "English",
		},
		{
			lang: "Ja" as const,
			text: "ã“ã‚“ã«ã¡ã¯ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒˆãƒ‹ãƒƒã‚¯APIãƒ¯ãƒ³ã§ã™ã€‚",
			label: "Japanese",
		},
		{
			lang: "Es" as const,
			text: "Hola, este es el modelo supertonic_api_1.",
			label: "Spanish",
		},
		{
			lang: "Pt" as const,
			text: "OlÃ¡, este Ã© o modelo supertonic_api_1.",
			label: "Portuguese",
		},
	];

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		let allPassed = true;
		const results: any[] = [];

		for (const tc of testCases) {
			console.log(`  ğŸ” Testing ${tc.label} (${tc.lang})...`);

			try {
				const langEnum =
					models.APIConvertTextToSpeechUsingCharacterRequestLanguage[tc.lang];

				const response = await client.textToSpeech.createSpeech({
					voiceId,
					apiConvertTextToSpeechUsingCharacterRequest: {
						text: tc.text,
						language: langEnum,
						outputFormat:
							models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat
								.Wav,
						model:
							models.APIConvertTextToSpeechUsingCharacterRequestModel
								.SupertonicApi1,
					},
				});

				console.log(`     âœ… ${tc.label} success`);
				results.push({ lang: tc.lang, success: true });
			} catch (e: any) {
				console.log(
					`     âŒ ${tc.label} failed: ${e.message?.substring(0, 50)}`
				);
				results.push({ lang: tc.lang, success: false, error: e.message });
				allPassed = false;
			}
		}

		console.log(
			`  ğŸ“Š Result: ${results.filter((r) => r.success).length}/${
				testCases.length
			} languages passed`
		);
		return [allPassed, results];
	} catch (e: any) {
		logDetailedError(e, "supertonic_api_1 multilingual");
		return [false, e];
	}
}

/**
 * Test unsupported language for sona_speech_1 (should fail with French)
 */
async function testUnsupportedLanguageSonaSpeech1(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log(
		"ğŸš« Unsupported Language Test - sona_speech_1 with French (Expected to Fail)"
	);

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” Attempting sona_speech_1 with French (unsupported)`);

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Bonjour, ceci est un test.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.Fr, // French - not supported by sona_speech_1
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				model:
					models.APIConvertTextToSpeechUsingCharacterRequestModel.SonaSpeech1,
			},
		});

		// If we reach here, the API didn't reject - may need server-side validation
		console.log(
			`  âš ï¸  API accepted the request - server-side validation may not enforce language restriction`
		);
		console.log(
			`  ğŸ“‹ Note: Language restriction may be enforced at API level, not SDK level`
		);
		return [
			true,
			{ note: "API accepted - language restriction may be server-side" },
		];
	} catch (e: any) {
		console.log(
			`  âœ… Correctly rejected unsupported language for sona_speech_1`
		);
		console.log(`  ğŸ“‹ Error: ${e.message?.substring(0, 100)}`);
		return [true, e];
	}
}

/**
 * Test unsupported language for supertonic_api_1 (should fail with German)
 */
async function testUnsupportedLanguageSupertonicApi1(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log(
		"ğŸš« Unsupported Language Test - supertonic_api_1 with German (Expected to Fail)"
	);

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” Attempting supertonic_api_1 with German (unsupported)`);

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Hallo, das ist ein Test.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.De, // German - not supported by supertonic_api_1
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				model:
					models.APIConvertTextToSpeechUsingCharacterRequestModel
						.SupertonicApi1,
			},
		});

		// If we reach here, the API didn't reject - may need server-side validation
		console.log(
			`  âš ï¸  API accepted the request - server-side validation may not enforce language restriction`
		);
		console.log(
			`  ğŸ“‹ Note: Language restriction may be enforced at API level, not SDK level`
		);
		return [
			true,
			{ note: "API accepted - language restriction may be server-side" },
		];
	} catch (e: any) {
		console.log(
			`  âœ… Correctly rejected unsupported language for supertonic_api_1`
		);
		console.log(`  ğŸ“‹ Error: ${e.message?.substring(0, 100)}`);
		return [true, e];
	}
}

/**
 * Test duration prediction with voice settings
 */
async function testPredictDurationWithVoiceSettings(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("â±ï¸  Duration Prediction with Voice Settings Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const voiceSettings = {
			speed: 0.8,
		};

		console.log(
			`  ğŸ” Predicting duration with voice settings for voice '${voiceId}'...`
		);
		console.log(`     Settings: speed=${voiceSettings.speed}`);

		const response = await client.textToSpeech.predictDuration({
			voiceId,
			predictTTSDurationUsingCharacterRequest: {
				text: "This is a duration test with adjusted speed.",
				language: models.PredictTTSDurationUsingCharacterRequestLanguage.En,
				voiceSettings,
			},
		});

		console.log(`  âœ… Predicted duration: ${response.duration}s`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test TTS streaming with voice settings
 */
async function testStreamSpeechWithVoiceSettings(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“¡ TTS Streaming with Voice Settings Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const voiceSettings = {
			pitchShift: 1.05,
			speed: 1.1,
		};

		console.log(
			`  ğŸ” Streaming speech with voice settings for voice '${voiceId}'...`
		);
		console.log(
			`     Settings: pitchShift=${voiceSettings.pitchShift}, speed=${voiceSettings.speed}`
		);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.streamSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Streaming with adjusted voice settings.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
				voiceSettings,
			},
		});

		console.log(`  âœ… Stream with voice settings started successfully`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test MP3 format TTS
 */
async function testCreateSpeechMp3(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ¤ MP3 Format TTS Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” MP3 TTS conversion with voice '${voiceId}'...`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Hello! This is an MP3 format SDK test. Let's verify if it works correctly.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Mp3,
				style: "neutral",
				model: "sona_speech_1",
			},
		});

		console.log(`  âœ… MP3 TTS conversion success`);

		if (response.result) {
			const outputFile = "test_create_speech_output.mp3";
			const audioData = await extractAudioData(response);

			fs.writeFileSync(outputFile, audioData);
			console.log(`  ğŸ’¾ MP3 audio file saved: ${outputFile}`);

			// Verify MP3 header
			const header = audioData.slice(0, 10);
			if (header[0] === 0x49 && header[1] === 0x44 && header[2] === 0x33) {
				console.log(`  âœ… Valid MP3 file generated (ID3 tag)`);
			} else if (
				(header[0] === 0xff && header[1] === 0xfb) ||
				(header[0] === 0xff && header[1] === 0xfa)
			) {
				console.log(`  âœ… Valid MP3 file generated (MPEG frame)`);
			} else {
				console.log(
					`  ğŸ“„ MP3 header: ${Array.from(header.slice(0, 10))
						.map((b) => b.toString(16).padStart(2, "0"))
						.join(" ")} (needs verification)`
				);
			}
		}

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test MP3 format with long text
 */
async function testCreateSpeechLongTextMp3(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“œ Long Text MP3 Auto-Chunking TTS Test (300+ chars)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const longText = `
		Hello! This is a very long text MP3 auto-chunking TTS test exceeding 300 characters.
		The newly implemented SDK automatically divides long text into multiple chunks for processing.
		Real-time streaming text-to-speech technology plays a crucial role in modern AI applications.
		It is an indispensable technology especially in conversational services, live broadcasting, and real-time translation services.
		Through the auto-chunking feature, long texts are naturally divided into multiple small segments for processing.
		Each segment is intelligently segmented considering sentence and word boundaries, enabling natural speech generation.
		Now users don't need to worry about text length or output format, as the SDK automatically handles everything in MP3 format too.
		`.trim();

		const actualLength = longText.length;
		console.log(
			`  ğŸ“ Test text length: ${actualLength} characters (exceeds 300)`
		);
		console.log(`  ğŸ”§ Auto-chunking enabled for MP3 format`);

		console.log(`  ğŸ” Converting long text to MP3 with voice '${voiceId}'...`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Mp3,
				style: "neutral",
				model: "sona_speech_1",
			},
		});

		console.log(`  âœ… MP3 auto-chunking TTS success`);

		if (response.result) {
			const outputFile = "test_auto_chunking_speech_output.mp3";
			const audioData = await extractAudioData(response);

			fs.writeFileSync(outputFile, audioData);
			console.log(`  ğŸ’¾ Auto-chunked MP3 audio file saved: ${outputFile}`);
		}

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "MP3 long text auto-chunking");
		return [false, e];
	}
}

/**
 * Test MP3 streaming
 */
async function testStreamSpeechMp3(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“¡ MP3 Streaming Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		console.log(`  ğŸ” Streaming MP3 speech with voice '${voiceId}'...`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.streamSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: "Testing MP3 streaming speech synthesis.",
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Mp3,
			},
		});

		console.log(`  âœ… MP3 stream started successfully`);

		return [true, response];
	} catch (e: any) {
		console.error(`  âŒ Error: ${e.message || e}`);
		return [false, e];
	}
}

/**
 * Test MP3 streaming with long text
 */
async function testStreamSpeechLongTextMp3(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“¡ Long Text MP3 Streaming Test");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		const longText = `
		Hello! This is a long text MP3 streaming test.
		The SDK automatically chunks and streams the MP3 audio in real-time.
		This enables efficient processing of longer content without waiting for complete generation.
		`
			.trim()
			.repeat(3);

		console.log(`  ğŸ” Streaming long text MP3 with voice '${voiceId}'...`);
		console.log(`     Text length: ${longText.length} characters`);
		console.log("  âš ï¸  This test consumes credits!");

		const response = await client.textToSpeech.streamSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Mp3,
			},
		});

		console.log(`  âœ… Long text MP3 stream started successfully`);

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "MP3 long text streaming");
		return [false, e];
	}
}

/**
 * Test TTS with automatic chunking (long text)
 * Note: Now createSpeech automatically handles chunking for long text
 */
async function testCreateSpeechWithChunking(
	voiceId: string | null
): Promise<[boolean, any]> {
	console.log("ğŸ“ TTS with Auto-Chunking Test (via createSpeech)");

	if (!voiceId) {
		console.log("  âš ï¸  No voice ID available");
		return [false, null];
	}

	try {
		const { Supertone } = await import("../src/index.js");
		const models = await import("../src/models/index.js");
		const client = new Supertone({ apiKey: API_KEY });

		// Long text that exceeds typical limits
		const longText =
			"This is a longer test text. ".repeat(20) +
			"It will be automatically chunked and merged.";

		console.log(`  ğŸ” Creating speech with auto-chunking`);
		console.log(`     Text length: ${longText.length} characters`);
		console.log("  âœ¨ Using createSpeech() - automatically chunks internally");
		console.log("  âš ï¸  This test consumes credits!");

		// Now just use regular createSpeech - it handles chunking automatically
		const response = await client.textToSpeech.createSpeech({
			voiceId,
			apiConvertTextToSpeechUsingCharacterRequest: {
				text: longText,
				language: models.APIConvertTextToSpeechUsingCharacterRequestLanguage.En,
				outputFormat:
					models.APIConvertTextToSpeechUsingCharacterRequestOutputFormat.Wav,
			},
		});

		console.log(`  âœ… Speech created and merged successfully`);
		console.log(`  ğŸ¯ Chunking handled automatically by SDK`);

		return [true, response];
	} catch (e: any) {
		logDetailedError(e, "Auto-chunking TTS");
		return [false, e];
	}
}

/**
 * Main test execution
 */
async function main(): Promise<boolean> {
	console.log("ğŸ§ª Real API Integration Test Start");
	console.log("=".repeat(60));
	console.log(
		"âš ï¸  WARNING: These tests make real API calls and consume credits!"
	);
	console.log("=".repeat(60));
	console.log("");

	const testResults: TestResult = {};
	const voiceIdForTTS: string = "91992bbd4758bdcf9c9b01";
	let customVoiceId: string | null = null;
	let createdCustomVoiceId: string | null = null;

	// 1. Usage Tests
	console.log("\nğŸ’° Usage & Credit Tests");
	console.log("-".repeat(60));

	let [success, result] = await testCreditBalance();
	testResults["credit_balance"] = success;

	[success, result] = await testGetUsage();
	testResults["get_usage"] = success;

	[success, result] = await testGetVoiceUsage();
	testResults["get_voice_usage"] = success;

	// 2. Voice Tests
	console.log("\nğŸµ Voice Tests");
	console.log("-".repeat(60));

	[success, result] = await testListVoices();
	testResults["list_voices"] = success;

	[success, result] = await testSearchVoices();
	testResults["search_voices"] = success;

	[success, result] = await testGetVoice(voiceIdForTTS);
	testResults["get_voice"] = success;

	// 3. Custom Voice Tests
	console.log("\nğŸ¨ Custom Voice Tests");
	console.log("-".repeat(60));

	[success, result] = await testListCustomVoices();
	testResults["list_custom_voices"] = success;
	if (success && result.voiceId) {
		customVoiceId = result.voiceId;
	}

	[success, result] = await testSearchCustomVoices();
	testResults["search_custom_voices"] = success;

	[success, result] = await testGetCustomVoice(customVoiceId);
	testResults["get_custom_voice"] = success;

	// 4. Custom Voice Creation/Edit/Delete Tests
	console.log("\nğŸ¨ Custom Voice Management Tests");
	console.log("-".repeat(60));
	console.log("âš ï¸  These tests consume credits and modify custom voices!");
	console.log("");

	[success, result] = await testCreateClonedVoice();
	testResults["create_cloned_voice"] = success;
	if (success && result?.voiceId) {
		createdCustomVoiceId = result.voiceId;
	}

	if (createdCustomVoiceId) {
		[success, result] = await testEditCustomVoice(createdCustomVoiceId);
		testResults["edit_custom_voice"] = success;

		console.log("\nâ¸ï¸  Pausing before deletion to allow voice to be used...");
		console.log(
			"âš ï¸  Note: In production, ensure voice is no longer needed before deletion"
		);

		[success, result] = await testDeleteCustomVoice(createdCustomVoiceId);
		testResults["delete_custom_voice"] = success;
	} else {
		console.log("â­ï¸  Skipping edit/delete tests (no custom voice created)");
		testResults["edit_custom_voice"] = null;
		testResults["delete_custom_voice"] = null;
	}

	// 5. TTS Basic Tests (only if voice ID available)
	if (voiceIdForTTS) {
		console.log("\nğŸ¤ Text-to-Speech Basic Tests");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume credits!");
		console.log("");

		[success, result] = await testPredictDuration(voiceIdForTTS);
		testResults["predict_duration"] = success;

		[success, result] = await testCreateSpeech(voiceIdForTTS);
		testResults["create_speech_wav"] = success;

		[success, result] = await testStreamSpeech(voiceIdForTTS);
		testResults["stream_speech"] = success;

		// 5.5 New Model Tests (sona_speech_2, supertonic_api_1)
		console.log("\nğŸ¤– New Model Tests (sona_speech_2, supertonic_api_1)");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume credits!");
		console.log("");

		[success, result] = await testCreateSpeechWithSonaSpeech2(voiceIdForTTS);
		testResults["create_speech_sona_speech_2"] = success;

		[success, result] = await testCreateSpeechWithSupertonicApi1(voiceIdForTTS);
		testResults["create_speech_supertonic_api_1"] = success;

		[success, result] = await testCreateSpeechWithUnsupportedModel(
			voiceIdForTTS
		);
		testResults["create_speech_unsupported_model"] = success;

		[success, result] = await testPredictDurationWithSonaSpeech2(voiceIdForTTS);
		testResults["predict_duration_sona_speech_2"] = success;

		[success, result] = await testPredictDurationWithSupertonicApi1(
			voiceIdForTTS
		);
		testResults["predict_duration_supertonic_api_1"] = success;

		[success, result] = await testPredictDurationWithUnsupportedModel(
			voiceIdForTTS
		);
		testResults["predict_duration_unsupported_model"] = success;

		// 5.6 Multilingual Tests per Model
		console.log("\nğŸŒ Multilingual Tests per Model");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume credits!");
		console.log("");

		[success, result] = await testMultilingualSonaSpeech1(voiceIdForTTS);
		testResults["multilingual_sona_speech_1"] = success;

		[success, result] = await testMultilingualSonaSpeech2(voiceIdForTTS);
		testResults["multilingual_sona_speech_2"] = success;

		[success, result] = await testMultilingualSupertonicApi1(voiceIdForTTS);
		testResults["multilingual_supertonic_api_1"] = success;

		// 5.7 Unsupported Language Tests
		console.log("\nğŸš« Unsupported Language Tests");
		console.log("-".repeat(60));
		console.log(
			"âš ï¸  These tests verify error handling for unsupported model-language combinations!"
		);
		console.log("");

		[success, result] = await testUnsupportedLanguageSonaSpeech1(voiceIdForTTS);
		testResults["unsupported_lang_sona_speech_1"] = success;

		[success, result] = await testUnsupportedLanguageSupertonicApi1(
			voiceIdForTTS
		);
		testResults["unsupported_lang_supertonic_api_1"] = success;

		// 6. TTS Long Text Tests
		console.log("\nğŸ“œ Text-to-Speech Long Text Tests");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume more credits!");
		console.log("");

		[success, result] = await testCreateSpeechLongText(voiceIdForTTS);
		testResults["create_speech_long_text"] = success;

		[success, result] = await testCreateSpeechLongSentenceNoPunctuation(
			voiceIdForTTS
		);
		testResults["create_speech_long_sentence_no_punctuation"] = success;

		[success, result] = await testCreateSpeechJapaneseNoSpaces(voiceIdForTTS);
		testResults["create_speech_japanese_no_spaces"] = success;

		[success, result] = await testStreamSpeechLongText(voiceIdForTTS);
		testResults["stream_speech_long_text"] = success;

		[success, result] = await testCreateSpeechWithChunking(voiceIdForTTS);
		testResults["create_speech_chunking"] = success;

		// 7. TTS with Voice Settings Tests
		console.log("\nğŸ›ï¸  Text-to-Speech with Voice Settings Tests");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume credits!");
		console.log("");

		[success, result] = await testCreateSpeechWithVoiceSettings(voiceIdForTTS);
		testResults["create_speech_voice_settings"] = success;

		[success, result] = await testPredictDurationWithVoiceSettings(
			voiceIdForTTS
		);
		testResults["predict_duration_voice_settings"] = success;

		[success, result] = await testStreamSpeechWithVoiceSettings(voiceIdForTTS);
		testResults["stream_speech_voice_settings"] = success;

		// 8. TTS with Phonemes Tests
		console.log("\nğŸ”¤ Text-to-Speech with Phonemes Tests");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume credits!");
		console.log("");

		[success, result] = await testCreateSpeechWithPhonemes(voiceIdForTTS);
		testResults["create_speech_phonemes"] = success;

		[success, result] = await testStreamSpeechWithPhonemes(voiceIdForTTS);
		testResults["stream_speech_phonemes"] = success;

		// 9. MP3 Format Tests
		console.log("\nğŸµ MP3 Format Tests");
		console.log("-".repeat(60));
		console.log("âš ï¸  These tests consume credits!");
		console.log("");

		[success, result] = await testCreateSpeechMp3(voiceIdForTTS);
		testResults["create_speech_mp3"] = success;

		[success, result] = await testCreateSpeechLongTextMp3(voiceIdForTTS);
		testResults["create_speech_long_text_mp3"] = success;

		[success, result] = await testStreamSpeechMp3(voiceIdForTTS);
		testResults["stream_speech_mp3"] = success;

		[success, result] = await testStreamSpeechLongTextMp3(voiceIdForTTS);
		testResults["stream_speech_long_text_mp3"] = success;
	}

	// Results Summary
	console.log("\n" + "=".repeat(60));
	console.log("ğŸ§ª Integration Test Results Summary:");
	console.log("");

	let passed = 0;
	let total = 0;

	for (const [testName, testResult] of Object.entries(testResults)) {
		let status: string;
		if (testResult === null) {
			status = "â­ï¸  SKIP";
		} else if (testResult) {
			status = "âœ… PASS";
			passed++;
			total++;
		} else {
			status = "âŒ FAIL";
			total++;
		}

		console.log(`  ${testName}: ${status}`);
	}

	console.log("");
	console.log(`Total ${passed}/${total} tests passed`);
	console.log("");

	if (passed === total) {
		console.log(
			"ğŸ‰ All integration tests passed! SDK works correctly with real API."
		);
		console.log("");
		console.log("âœ… SDK ready for deployment!");
	} else {
		console.log("âš ï¸  Some tests failed. Please check:");
		console.log("  â€¢ API key is valid");
		console.log("  â€¢ Account has sufficient credits");
		console.log("  â€¢ Network connection is stable");
	}

	console.log("");
	console.log("ğŸ“‹ Tested APIs:");
	console.log("  â€¢ Usage: getCreditBalance, getUsage, getVoiceUsage");
	console.log("  â€¢ Voices: listVoices, searchVoices, getVoice");
	console.log(
		"  â€¢ Custom Voices: listCustomVoices, searchCustomVoices, getCustomVoice"
	);
	console.log(
		"  â€¢ Custom Voice Management: createClonedVoice, editCustomVoice, deleteCustomVoice"
	);
	console.log(
		"  â€¢ Text-to-Speech: predictDuration, createSpeech, streamSpeech"
	);
	console.log("  â€¢ TTS Long Text: createSpeechLongText, streamSpeechLongText");
	console.log(
		"  â€¢ TTS Chunking Strategies: Word-based (no punctuation), Character-based (Japanese)"
	);
	console.log(
		"  â€¢ TTS with Voice Settings: createSpeechWithVoiceSettings, predictDurationWithVoiceSettings, streamSpeechWithVoiceSettings"
	);
	console.log(
		"  â€¢ TTS with Phonemes: createSpeechWithPhonemes, streamSpeechWithPhonemes"
	);
	console.log(
		"  â€¢ MP3 Format: createSpeechMp3, createSpeechLongTextMp3, streamSpeechMp3, streamSpeechLongTextMp3"
	);
	console.log(
		"  â€¢ Custom Features: Auto-chunking in createSpeech/streamSpeech (transparent)"
	);
	console.log("");
	console.log("ğŸ¤– New Model & Language Tests:");
	console.log(
		"  â€¢ New Models: sona_speech_2, supertonic_api_1 (createSpeech & predictDuration)"
	);
	console.log(
		"  â€¢ Unsupported Model Validation: Error handling for invalid model names"
	);
	console.log("  â€¢ Multilingual per Model:");
	console.log("    - sona_speech_1: ko, en, ja");
	console.log("    - sona_speech_2: all 23 languages");
	console.log("    - supertonic_api_1: ko, en, ja, es, pt");
	console.log(
		"  â€¢ Unsupported Language Validation: Error handling for invalid model-language combinations"
	);

	if (customVoiceId) {
		console.log("");
		console.log(`ğŸ¨ Found existing custom voice: ${customVoiceId}`);
	}

	if (createdCustomVoiceId) {
		console.log(`ğŸ†• Created and deleted custom voice: ${createdCustomVoiceId}`);
	}

	console.log("");
	console.log("ğŸ’¡ Note: This is a comprehensive test suite.");
	console.log(
		"   Excludes parallel processing tests (see Python version for those)"
	);

	return passed === total;
}

// Run tests
main()
	.then((success) => {
		process.exit(success ? 0 : 1);
	})
	.catch((error) => {
		console.error("âŒ Test execution failed:", error);
		process.exit(1);
	});
